\documentclass[UTF8]{ctexart}
\usepackage{amsmath}
\usepackage{graphicx}
\usepackage{float}
\usepackage{subfigure}
\usepackage{xeCJK}
\usepackage{hyperref}
\usepackage{algorithm2e}
\usepackage{amsfonts}
\usepackage{epsfig}

\graphicspath{{images/}}
\setCJKmonofont{Microsoft YaHei}

\title{\Huge{《统计学习方法》(李航)\\归纳总结三\\K近邻法\\} \huge{西安交通大学 \\ 联系方式：williamyi96@gmail.com\\ }}
\author{\huge{易凯}}
\date{\Huge\today}

\begin{document}
	\maketitle
	\vspace{100mm}
	\newpage
	\tableofcontents
	\newpage

	\section{K近邻法}
	\subsection{K近邻算法}
	K近邻算法是基本而且简单的分类和回归方法。其基本思路是首先确定输入实例点的k个最近邻实例点，然后利用这k个近邻的实例点采用诸如多数表决的方式决定该点所属的类。

	\subsection{K近邻模型}
	\paragraph{K近邻法三要素：} 
	距离度量、k值选择、分类决策规则

	\paragraph{模型}
	K近邻模型对应于基于训练数据集对特征空间的一个划分。K近邻法中，当训练集、距离度量、k值和分类策略规则确定之后，其结果就具有了唯一性。

	\paragraph{距离测量}
	常用的距离测量方法有欧氏距离法和Lp距离法。

	\paragraph{K值选择}
	K值的选择要适当，过小噪声引起的估计误差会增大；过大的分类本身有不具备代表性。K值在数据量有限的情况下一般使用交叉验证法进行确定。

	\paragraph{分类决策规则}
	分类决策一般采取多数表决的形式，其对应的是经验风险的最小化。

	\subsection{K近邻法的实现：KD树}
	K近邻法的实现需要考虑如何快速搜索K个最近邻点。KD树是一种便于对K维空间中的数据进行快速检索的数据结构。

	同时，KD树是一棵二叉树，表示的是对维空间的一个划分，其每个结点对应于K维空间划分中的一个超矩形区域。

	利用KD树可以省去大部分数据点的搜索，从而减少搜索的计算量。
\end{document}